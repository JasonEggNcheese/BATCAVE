{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3DDnfxOiY7dIN5rfLs4Ly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonEggNcheese/BATCAVE/blob/main/ERICALLMAAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s7Q_HfiWdQ2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "-W_-XTMsYPtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export GROQ_API_KEY= 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\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "kafKDKQGYRdi",
        "outputId": "286208c7-fdb9-48d9-b2e2-d6ad256cb3df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-54ff760c0892>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-54ff760c0892>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    export GROQ_API_KEY= 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\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGi-Jby3YoIU",
        "outputId": "d8aef905-7159-4b30-f24e-2dac6cf9d845"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq"
      ],
      "metadata": {
        "id": "zqIfAobzZgX5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "Ca4FonMiZpZ5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = Groq(\n",
        "    api_key=\"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\"\n",
        ")"
      ],
      "metadata": {
        "id": "4GGKRLrkZ2pZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTEaqP3AaFGq",
        "outputId": "8efa1bbd-0f69-4714-9658-8eab9cb8e0a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models have become increasingly important in recent years due to their numerous applications and advantages in natural language processing (NLP) tasks. Here are some reasons why fast language models are important:\n",
            "\n",
            "1. **Real-time processing**: Fast language models can process text data in real-time, making them essential for applications that require immediate responses, such as chatbots, virtual assistants, and real-time translation software.\n",
            "2. **Scalability**: As the amount of text data grows, traditional language models struggle to scale efficiently. Fast language models can handle large amounts of data and compute tasks in parallel, making them better suited for large-scale applications.\n",
            "3. **Faster inference**: Fast language models can perform inference (i.e., generate text or answer queries) much faster than traditional language models. This is crucial for applications that require fast response times, such as search engines, recommendation systems, and dialogue systems.\n",
            "4. **Efficient memory usage**: Fast language models are designed to use memory efficiently, which is essential for deploying models on devices with limited memory, such as smartphones, smart home devices, or edge computing devices.\n",
            "5. **Improved search and recommendation**: Fast language models can quickly scan large datasets to retrieve relevant information or generate recommendations, making them useful for search engines, e-commerce platforms, and content recommendation systems.\n",
            "6. **Enhanced conversational AI**: Fast language models are crucial for conversational AI applications, such as virtual assistants, chatbots, and language translators, which require real-time processing and accurate responses.\n",
            "7. **Improved accuracy**: Fast language models often use techniques like parallel processing, distributed computing, and specialized hardware (e.g., GPUs, TPUs) to improve the accuracy of their predictions and inferences.\n",
            "8. **Support for multimodal input**: Fast language models can handle multimodal input, such as text, images, and audio, making them useful for applications like visual question answering, image captioning, and speech-to-text systems.\n",
            "9. **Simplified development**: Fast language models can simplify the development process for NLP applications by providing pre-trained models and simplified APIs, allowing developers to focus on their specific use cases rather than building a language model from scratch.\n",
            "10. **Breakthroughs in NLP research**: The development of fast language models has driven progress in NLP research, enabling the creation of more complex and accurate models that can tackle previously unsolvable tasks.\n",
            "\n",
            "Some examples of fast language models include:\n",
            "\n",
            "* BERT (Bidirectional Encoder Representations from Transformers)\n",
            "* RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
            "* DistilBERT (Distilled BERT)\n",
            "* Longformer (a long-range dependency transformer-based model)\n",
            "* BigBird (a larger-than-BERT language model)\n",
            "\n",
            "These models have demonstrated significant improvements in various NLP tasks, such as language understanding, text classification, sentiment analysis, and machine translation. They have also been widely adopted in various applications, including customer service chatbots, search engines, and language translation software.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llama_70B = \"llama-3.1-70b-versatile\"\n",
        "llama_405B = \"llama-3.1-405b-reasoning\""
      ],
      "metadata": {
        "id": "cBMtKvnlahat"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "1GBov_asbapB",
        "outputId": "b7276d1d-c60c-4035-d981-95afcc0f445a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': 'max_tokens for `llama-3.1-70b-versatile` must be less than or equal to 8000', 'type': 'invalid_request_error'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-658595bd0777>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     messages=[\n\u001b[1;32m      3\u001b[0m         {\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Explain the importance of fast language models\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    287\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 920\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    921\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1018\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'max_tokens for `llama-3.1-70b-versatile` must be less than or equal to 8000', 'type': 'invalid_request_error'}}"
          ]
        }
      ]
    },
    {
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    max_tokens=8000  # Set max_tokens to the maximum allowed value\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2_TYXFEbpZ6",
        "outputId": "a90767bc-48b8-4ee4-e13d-7c33bd63fcae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are a crucial aspect of Natural Language Processing (NLP) and have numerous applications in various industries. Here are some reasons why fast language models are important:\n",
            "\n",
            "1. **Efficient Processing**: Fast language models can process large amounts of text data quickly, making them essential for real-time applications such as chatbots, sentiment analysis, and language translation.\n",
            "2. **Improved User Experience**: Fast language models enable faster response times, which is critical for applications that require instant feedback, such as virtual assistants, customer support, and language understanding systems.\n",
            "3. **Scalability**: Fast language models can handle a large volume of requests without significant performance degradation, making them ideal for large-scale applications, such as social media monitoring, text classification, and information retrieval.\n",
            "4. **Real-time Decision Making**: Fast language models can analyze text data in real-time, enabling applications that require immediate decision making, such as spam detection, sentiment analysis, and anomaly detection.\n",
            "5. **Reducing Latency**: Fast language models reduce latency, which is critical for applications that require quick responses, such as search engines, question-answering systems, and language translation.\n",
            "6. **Increased Productivity**: Fast language models can automate tasks that require human-level language understanding, freeing up human resources for more complex tasks, such as content creation, editing, and proofreading.\n",
            "7. **Competitive Advantage**: Fast language models can provide a competitive advantage in industries where speed and accuracy are critical, such as finance, healthcare, and customer service.\n",
            "8. **Enhanced Research and Development**: Fast language models enable researchers to explore new applications and domains, accelerating innovation in NLP and related fields.\n",
            "\n",
            "Some specific use cases where fast language models are particularly important:\n",
            "\n",
            "1. **Conversational AI**: Fast language models are essential for building conversational AI systems that can engage in real-time conversations with humans.\n",
            "2. **Language Translation**: Fast language models enable real-time language translation, breaking down language barriers and facilitating global communication.\n",
            "3. **Sentiment Analysis**: Fast language models can analyze text data in real-time, enabling instant sentiment analysis and decision making.\n",
            "4. **Text Classification**: Fast language models can classify text data quickly, enabling applications such as spam detection, content moderation, and information retrieval.\n",
            "5. **Virtual Assistants**: Fast language models power virtual assistants, such as Siri, Alexa, and Google Assistant, which require rapid language understanding and response.\n",
            "\n",
            "To achieve fast language models, researchers and developers employ various techniques, such as:\n",
            "\n",
            "1. **Model Pruning**: Removing redundant weights and connections to reduce model size and increase speed.\n",
            "2. **Quantization**: Representing model weights and activations using fewer bits to reduce memory usage and increase speed.\n",
            "3. **Knowledge Distillation**: Transferring knowledge from large, slow models to smaller, faster models.\n",
            "4. **Parallel Processing**: Utilizing multiple processing units or GPUs to accelerate model computations.\n",
            "\n",
            "Overall, fast language models are crucial for building efficient, scalable, and responsive NLP applications that can process and analyze text data in real-time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"the top 15 beef tip recipes\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.1-70b-versatile\",\n",
        "    max_tokens=8000  # Set max_tokens to the maximum allowed value\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORW306L7dKEh",
        "outputId": "ba94defb-07fa-4cfa-8fb8-34c946e68375"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A delicious topic! Here are 15 mouth-watering beef tip recipes for you to try:\n",
            "\n",
            "**1. Classic Beef Tips in Gravy**: A comfort food staple made with beef tips, mushrooms, and a rich, savory gravy served over egg noodles.\n",
            "\n",
            "**2. Asian-Style Beef Tips**: Marinated beef tips cooked in a flavorful stir-fry sauce, served with rice and stir-fried vegetables.\n",
            "\n",
            "**3. Creamy Mushroom Beef Tips**: Tender beef tips smothered in a rich and creamy mushroom sauce, served over mashed potatoes or egg noodles.\n",
            "\n",
            "**4. Italian-Style Beef Tips**: Braised beef tips in a tangy tomato-based sauce, served with garlic bread and sautéed spinach.\n",
            "\n",
            "**5. Mexican Beef Tips Tacos**: Seasoned beef tips cooked in a spicy taco sauce, served in tacos with fresh cilantro, onions, and tortillas.\n",
            "\n",
            "**6. Horseradish Beef Tips**: A creamy horseradish sauce pairs perfectly with beef tips, served over a bed of fresh arugula and garlic bread.\n",
            "\n",
            "**7. Steakhouse-Style Beef Tips**: A gourmet recipe featuring pan-seared beef tips in a Cabernet reduction, served with garlic mashed potatoes and sautéed broccoli.\n",
            "\n",
            "**8. Beer Braised Beef Tips**: Tender beef tips slow-cooked in a flavorful beer broth, served with crispy potatoes and carrots.\n",
            "\n",
            "**9. Stroganoff-Style Beef Tips**: Sauteed beef tips, mushrooms, and a tangy cream sauce served over egg noodles, perfect for a comforting weeknight dinner.\n",
            "\n",
            "**10. Honey Bourbon Beef Tips**: Sweet and savory, this recipe combines honey and bourbon with tender beef tips, served with garlic rice and steamed green beans.\n",
            "\n",
            "**11. Sizzling Skillet Beef Tips**: Quickly cooked beef tips, onions, and bell peppers, served with a flavorful slaw and crispy corn tortillas.\n",
            "\n",
            "**12. Worcestershire Beef Tips**: Classic British-style beef tips, made with a pungent Worcestershire sauce and served over boiled potatoes and green beans.\n",
            "\n",
            "**13. Tomato Beef Tips Skillet**: A one-pot wonder made with sautéed beef tips, tomatoes, onions, and fresh herbs, perfect for a simple, hearty meal.\n",
            "\n",
            "**14. Hoisin-Glazed Beef Tips**: Asian-inspired beef tips, smothered in a sticky and sweet hoisin sauce, served over a bed of garlic mashed potatoes and roasted carrots.\n",
            "\n",
            "**15. Blue Cheese Crusted Beef Tips**: Beef tips smothered in a tangy blue cheese sauce and baked to a golden, cheesy perfection, perfect as a hearty appetizer or entree.\n",
            "\n",
            "Hope these beef tip recipes spark your creativity in the kitchen!\n"
          ]
        }
      ]
    }
  ]
}